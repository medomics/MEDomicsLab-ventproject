{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp trainutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.vision.all import *\n",
    "from fastai2.torch_imports import *\n",
    "from fastai2.torch_core import *\n",
    "# from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export    \n",
    "# def dice_loss(logits, target, smooth=1.):\n",
    "#     if torch.any(torch.isnan(logits)): print(\"logits contain nan\")\n",
    "#     probas = torch.sigmoid(logits)\n",
    "#     if torch.any(torch.isnan(probas)): print(\"probas contain nan\")\n",
    "#     iflat = probas.view(-1)\n",
    "#     tflat = target.view(-1)\n",
    "#     intersection = (iflat * tflat).sum()\n",
    "#     return 1 - ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "def bce(input, target):\n",
    "    bs = input.shape[0]\n",
    "    return F.binary_cross_entropy_with_logits(input.view(bs,-1).float(), target.view(bs,-1).float())\n",
    "\n",
    "def dice_loss(logits, target, smooth=1.):\n",
    "    logits = torch.sigmoid(logits)\n",
    "    iflat = logits.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return 1 - ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, logits, target):\n",
    "        logits = logits.squeeze(1)\n",
    "        if not (target.size() == logits.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), logits.size()))\n",
    "\n",
    "        max_val = (-logits).clamp(min=0)\n",
    "        loss = logits - logits * target + max_val + \\\n",
    "            ((-max_val).exp() + (-logits - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-logits * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.mean()\n",
    "    \n",
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.focal = FocalLoss(gamma)\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        loss = self.alpha*self.focal(input, target) - torch.log(1 - dice_loss(input, target))\n",
    "        return loss.mean()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.layers import *\n",
    "loss_dict = {'dice':dice_loss, 'bce':BCEWithLogitsLossFlat(), 'mixed':MixedLoss(10., 2.)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SaveModelCallback(TrackerCallback):\n",
    "#     \"SaveModelCallback modified for distributed transfer learning\"\n",
    "#     def __init__(self, learn:Learner, monitor:str='val_loss', mode:str='auto', every:str='improvement', name:str='bestmodel', best_init=None):\n",
    "#         super().__init__(learn, monitor=monitor, mode=mode)\n",
    "#         self.every,self.name = every,name\n",
    "#         if self.every not in ['improvement', 'epoch']:\n",
    "#             warn(f'SaveModel every {self.every} is invalid, falling back to \"improvement\".')\n",
    "#             self.every = 'improvement'\n",
    "#         if best_init: self.best = best_init \n",
    "      \n",
    "#     def on_train_begin(self, **kwargs:Any)->None:\n",
    "#         \"Initializes the best value.\"\n",
    "#         if not hasattr(self, 'best'):\n",
    "#             self.best = float('inf') if self.operator == np.less else -float('inf')\n",
    "# #         print('best init score:', self.best) \n",
    "        \n",
    "#     def jump_to_epoch(self, epoch:int)->None:\n",
    "#         try: \n",
    "#             self.learn.load(f'{self.name}_{epoch-1}', purge=False)\n",
    "#             print(f\"Loaded {self.name}_{epoch-1}\")\n",
    "#         except: print(f'Model {self.name}_{epoch-1} not found.')\n",
    "\n",
    "#     def on_epoch_end(self, epoch:int, **kwargs:Any)->None:\n",
    "#         \"Compare the value monitored to its best score and maybe save the model.\"\n",
    "#         if self.every==\"epoch\": self.learn.save(f'{self.name}_{epoch}')\n",
    "#         else: #every=\"improvement\"\n",
    "#             current = self.get_monitor_value()\n",
    "#             if current is not None and self.operator(current, self.best):\n",
    "#                 print(f'Better model found at epoch {epoch} with {self.monitor} value: {current}.')\n",
    "#                 self.best = current\n",
    "#                 self.learn.save(f'{self.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n",
    "gnorm_types = (nn.GroupNorm,)\n",
    "insnorm_types = (nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d)\n",
    "norm_types = bn_types + gnorm_types + insnorm_types\n",
    "\n",
    "def my_cond_init(m,func):\n",
    "    \"Initialize the non-batchnorm and PRelu layers of `m` with `init_func`.\"\n",
    "    if (not isinstance(m, (*norm_types, nn.PReLU))) and requires_grad(m): init_default(m, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BackwardHookCallback(LearnerCallback):\n",
    "#     \"Callback that can be used to register hooks on `modules`. Implement the corresponding function in `self.hook`.\"\n",
    "#     def __init__(self, learn:Learner, modules:Sequence[nn.Module]=None, do_remove:bool=True):\n",
    "#         super().__init__(learn)\n",
    "#         self.modules,self.do_remove = modules,do_remove\n",
    "\n",
    "#     def on_train_begin(self, **kwargs):\n",
    "#         \"Register the `Hooks` on `self.modules`.\"\n",
    "#         if not self.modules:\n",
    "#             self.modules = [m for m in flatten_model(self.learn.model)\n",
    "#                             if hasattr(m, 'weight')]\n",
    "#         # needs to be is_forward=False, detach=False\n",
    "#         self.hooks = Hooks(self.modules, self.hook, is_forward=False, detach=False)\n",
    "\n",
    "#     def on_train_end(self, **kwargs):\n",
    "#         \"Remove the `Hooks`.\"\n",
    "#         if self.do_remove: self.remove()\n",
    "\n",
    "#     def remove(self): \n",
    "#         if getattr(self, 'hooks', None): self.hooks.remove()\n",
    "    \n",
    "#     def __del__(self): self.remove()\n",
    "        \n",
    "        \n",
    "# class CatchNanGrad(BackwardHookCallback):\n",
    "#     \"Catch NaN when first appears in grad\"\n",
    "\n",
    "#     def on_train_begin(self, **kwargs):\n",
    "#         super().on_train_begin(**kwargs)\n",
    "#         self.stop = False\n",
    "\n",
    "#     def hook(self, m:nn.Module, i:Tensors, o:Tensors)->Tuple[Rank0Tensor,Rank0Tensor]:\n",
    "#         \"Take the mean and std of `o`.\"\n",
    "#         if (not self.stop) and (torch.any(torch.isnan(o[0]))): \n",
    "#             self.stop = True\n",
    "#             print(m,o)\n",
    "#             return (m, o)\n",
    "    \n",
    "#     def on_backward_end(self, train, epoch, num_batch, **kwargs):\n",
    "#         \"Called after backprop but before optimizer step.\"\n",
    "#         if train and self.stop: \n",
    "#             print (f'Epoch/Batch ({epoch}/{num_batch}): Invalid Grad, terminating training.')\n",
    "#             return {'stop_epoch': True, 'stop_training': True, 'skip_validate': True}\n",
    "        \n",
    "# class CatchNanActs(HookCallback):\n",
    "#     \"Catch NaN when first appears in acts\"\n",
    "\n",
    "#     def on_train_begin(self, **kwargs):\n",
    "#         super().on_train_begin(**kwargs)\n",
    "#         self.stop = False\n",
    "\n",
    "#     def hook(self, m:nn.Module, i:Tensors, o:Tensors)->Tuple[Rank0Tensor,Rank0Tensor]:\n",
    "#         \"Take the mean and std of `o`.\"\n",
    "#         if (not self.stop) and (torch.any(torch.isnan(o[0]))): \n",
    "#             self.stop = True\n",
    "#             print(m,o)\n",
    "#             return (m, o)\n",
    "            \n",
    "#     def on_loss_begin(self, train, epoch, num_batch, **kwargs):\n",
    "#         \"Called after forward pass but before loss has been computed.\"\n",
    "#         if train and self.stop: \n",
    "#             print (f'Epoch/Batch ({epoch}/{num_batch}): Invalid Activation, terminating training.')\n",
    "#             return {'stop_epoch': True, 'stop_training': True, 'skip_validate': True}\n",
    "        \n",
    "# class CatchNanParameters(LearnerCallback):\n",
    "#     \"Catch NaN when first appears in parameter weight\"\n",
    "    \n",
    "#     def on_train_begin(self, **kwargs):\n",
    "#         super().on_train_begin(**kwargs)\n",
    "#         self.modules = [m for m in flatten_model(self.learn.model) if hasattr(m, 'weight')]\n",
    "        \n",
    "#     def on_backward_end(self, train, epoch, num_batch, **kwargs):\n",
    "#         for m in self.modules: \n",
    "#             if m.weight is not None:\n",
    "#                 if torch.any(torch.isnan(m.weight)): \n",
    "#                     print (f'Epoch/Batch ({epoch}/{num_batch}): Invalid Parameter, terminating training.')\n",
    "#                     print(m,o)\n",
    "#                     return {'stop_epoch': True, 'stop_training': True, 'skip_validate': True}\n",
    "                \n",
    "# class ActStats:\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "#     def __call__(self, m, i, o):\n",
    "#         d = o.data\n",
    "\n",
    "#         self.mean,self.std = d.mean().item(),d.std().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lsuv_module(m, model, xb):\n",
    "#     stats = ActStats()\n",
    "#     h = Hook(m, stats)\n",
    "\n",
    "#     if hasattr(m, 'bias'): \n",
    "#         while model(xb) is not None and abs(stats.mean)  > 1e-3: m.bias -= stats.mean\n",
    "#     if hasattr(m, 'weight'):\n",
    "#         while model(xb) is not None and abs(stats.std-1) > 1e-3: m.weight.data /= stats.std\n",
    "\n",
    "#     h.remove()\n",
    "#     return stats.mean, stats.std\n",
    "\n",
    "# def lsuv_init(learn):\n",
    "#     \"initialize model parameters with LSUV - https://arxiv.org/abs/1511.06422\"\n",
    "#     modules = [m for m in flatten_model(learn.model) if hasattr(m, 'weight') and m.weight is not None]\n",
    "#     mdl = learn.model.cuda()\n",
    "#     xb, yb = learn.data.one_batch()\n",
    "#     for m in modules: print(lsuv_module(m, learn.model, xb.cuda()))\n",
    "#     for m in modules: \n",
    "#         if hasattr(m, 'weight'): assert not torch.any(torch.isnan(m.weight))\n",
    "#         if hasattr(m, 'bias'): assert not torch.any(torch.isnan(m.bias))\n",
    "#     del modules\n",
    "#     del mdl\n",
    "#     del xb\n",
    "#     del yb\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#metrics\n",
    "# def get_img_pred_masks(learner, dl, thresh=0.5):\n",
    "#     model = learner.model.eval()\n",
    "#     preds = []\n",
    "#     images = []\n",
    "#     masks = []\n",
    "#     for image, mask in tqdm_notebook(dl):\n",
    "#         out = model(image.half().cuda())\n",
    "#         out = torch.sigmoid(out).cpu().data.numpy()\n",
    "#         out = out.astype(float)\n",
    "#         out = (out > thresh)*1\n",
    "#         image = image.cpu().data.numpy()\n",
    "#         mask = mask.cpu().data.numpy()\n",
    "#         for i in range(out.shape[0]):\n",
    "#             masks.append(mask[i])\n",
    "#             preds.append(out[i][0])\n",
    "#             images.append(image[i][0])\n",
    "#     return images, preds, masks\n",
    "\n",
    "# def plot_predictions(true, pred, mask):\n",
    "#     \"\"\"draw image, pred, mask side by side\"\"\"\n",
    "#     fig, ax = plt.subplots(1,3, figsize=(20,10))\n",
    "#     axes = ax.flatten()\n",
    "#     for ax, im, t in zip(axes, [true, pred, mask], [\"image\", \"pred\", \"mask\"]) :\n",
    "#         ax.imshow(im, cmap=\"gray\")\n",
    "#         ax.set_title(t, fontdict={\"fontsize\":20})\n",
    "\n",
    "def dice_score(logits, targets, thresh=0.5):\n",
    "    hard_preds = torch.sigmoid(logits) > thresh\n",
    "    m1 = hard_preds.view(-1).float() \n",
    "    m2 = targets.view(-1).float()\n",
    "    intersection = (m1 * m2).sum()\n",
    "    return (2. * intersection) / (m1.sum() + m2.sum() + 1e-6)\n",
    "\n",
    "# def eval_preds(preds, target, thresh=0.5):\n",
    "#     iflat = np.array(preds).reshape(-1)\n",
    "#     tflat = np.array(target).reshape(-1)\n",
    "#     intersection = (iflat * tflat).sum()\n",
    "#     return ((2.0 * intersection) / (iflat.sum() + tflat.sum() + 1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 3a) trainutils.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from local.notebook.export import notebook2script\n",
    "notebook2script(\"3a) trainutils.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai_dev]",
   "language": "python",
   "name": "conda-env-fastai_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp traintransfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set `data.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['YAML_DATA']=\"/home/turgutluk/Vent_Seg_Project/dev/configs/data.yaml\"\n",
    "os.environ['YAML_TL']=\"/home/turgutluk/Vent_Seg_Project/dev/configs/transfer_learning.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.environ.get('YAML_DATA', '../data.yaml')) as f: data_config = yaml.load(f.read(), yaml.FullLoader)\n",
    "with open(os.environ.get('YAML_TL', 'transfer_learning.yaml')) as f: tl_config = yaml.load(f.read(), yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BRAIN': {'MR': {'TL_Brain_MR_Baseline_1': 'best_of_ATLAS_Brain_MR_Baseline_1',\n",
       "   'TL_Brain_MR_Baseline_2': 'best_of_ATLAS_Brain_MR_Baseline_2',\n",
       "   'TL_Brain_MR_Baseline_3': 'best_of_ATLAS_Brain_MR_Baseline_3',\n",
       "   'TL_Brain_MR_Baseline_4': 'best_of_ATLAS_Brain_MR_Baseline_4',\n",
       "   'TL_Brain_MR_Baseline_5': 'best_of_ATLAS_Brain_MR_Baseline_5',\n",
       "   'TL_Brain_MR_Baseline_6': 'best_of_ATLAS_Brain_MR_Baseline_6',\n",
       "   'TL_Brain_MR_Baseline_7': 'best_of_ATLAS_Brain_MR_Baseline_7',\n",
       "   'TL_Brain_MR_Baseline_8': 'best_of_ATLAS_Brain_MR_Baseline_8',\n",
       "   'TL_Brain_MR_Baseline_9': 'best_of_ATLAS_Brain_MR_Baseline_9',\n",
       "   'TL_Brain_MR_Baseline_10': 'best_of_ATLAS_Brain_MR_Baseline_10',\n",
       "   'TL_Brain_MR_Baseline_11': 'best_of_ATLAS_Brain_MR_Baseline_11'},\n",
       "  'CT': {'TL_Brain_CT_Baseline_1': 'best_of_ATLAS_Brain_MR_Baseline_1',\n",
       "   'TL_Brain_CT_Baseline_2': 'best_of_ATLAS_Brain_MR_Baseline_2',\n",
       "   'TL_Brain_CT_Baseline_3': 'best_of_ATLAS_Brain_MR_Baseline_3',\n",
       "   'TL_Brain_CT_Baseline_4': 'best_of_ATLAS_Brain_MR_Baseline_4',\n",
       "   'TL_Brain_CT_Baseline_5': 'best_of_ATLAS_Brain_MR_Baseline_5',\n",
       "   'TL_Brain_CT_Baseline_6': 'best_of_ATLAS_Brain_MR_Baseline_6',\n",
       "   'TL_Brain_CT_Baseline_7': 'best_of_ATLAS_Brain_MR_Baseline_7',\n",
       "   'TL_Brain_CT_Baseline_8': 'best_of_ATLAS_Brain_MR_Baseline_8',\n",
       "   'TL_Brain_CT_Baseline_9': 'best_of_ATLAS_Brain_MR_Baseline_9',\n",
       "   'TL_Brain_CT_Baseline_10': 'best_of_ATLAS_Brain_MR_Baseline_10',\n",
       "   'TL_Brain_CT_Baseline_11': 'best_of_ATLAS_Brain_MR_Baseline_11'}},\n",
       " 'VENTRICLE': {'MR': {'TL_Ventricle_MR_Baseline_1': 'best_of_ATLAS_Ventricle_MR_Baseline_1',\n",
       "   'TL_Ventricle_MR_Baseline_2': 'best_of_ATLAS_Ventricle_MR_Baseline_2',\n",
       "   'TL_Ventricle_MR_Baseline_3': 'best_of_ATLAS_Ventricle_MR_Baseline_3',\n",
       "   'TL_Ventricle_MR_Baseline_4': 'best_of_ATLAS_Ventricle_MR_Baseline_4',\n",
       "   'TL_Ventricle_MR_Baseline_5': 'best_of_ATLAS_Ventricle_MR_Baseline_5',\n",
       "   'TL_Ventricle_MR_Baseline_6': 'best_of_ATLAS_Ventricle_MR_Baseline_6',\n",
       "   'TL_Ventricle_MR_Baseline_7': 'best_of_ATLAS_Ventricle_MR_Baseline_7',\n",
       "   'TL_Ventricle_MR_Baseline_8': 'best_of_ATLAS_Ventricle_MR_Baseline_8',\n",
       "   'TL_Ventricle_MR_Baseline_9': 'best_of_ATLAS_Ventricle_MR_Baseline_9',\n",
       "   'TL_Ventricle_MR_Baseline_10': 'best_of_ATLAS_Ventricle_MR_Baseline_10',\n",
       "   'TL_Ventricle_MR_Baseline_11': 'best_of_ATLAS_Ventricle_MR_Baseline_11',\n",
       "   'TL_Ventricle_MR_Samples10_Baseline_9': 'best_of_ATLAS_Ventricle_MR_Baseline_9',\n",
       "   'TL_Ventricle_MR_Samples20_Baseline_9': 'best_of_ATLAS_Ventricle_MR_Baseline_9',\n",
       "   'TL_Ventricle_MR_Samples50_Baseline_9': 'best_of_ATLAS_Ventricle_MR_Baseline_9',\n",
       "   'TL_Ventricle_MR_Baseline_9_5_SAMPLES': 'best_of_ATLAS_Ventricle_MR_Baseline_9',\n",
       "   'TL_Ventricle_MR_Baseline_9_10_SAMPLES': 'best_of_ATLAS_Ventricle_MR_Baseline_9',\n",
       "   'TL_Ventricle_MR_Baseline_9_20_SAMPLES': 'best_of_ATLAS_Ventricle_MR_Baseline_9',\n",
       "   'TL_Ventricle_MR_Baseline_9_40_SAMPLES': 'best_of_ATLAS_Ventricle_MR_Baseline_9',\n",
       "   'TL_Ventricle_MR_Baseline_9_60_SAMPLES': 'best_of_ATLAS_Ventricle_MR_Baseline_9',\n",
       "   'TL_Ventricle_MR_Baseline_11_5_SAMPLES': 'best_of_ATLAS_Ventricle_MR_Baseline_11',\n",
       "   'TL_Ventricle_MR_Baseline_11_10_SAMPLES': 'best_of_ATLAS_Ventricle_MR_Baseline_11',\n",
       "   'TL_Ventricle_MR_Baseline_11_20_SAMPLES': 'best_of_ATLAS_Ventricle_MR_Baseline_11',\n",
       "   'TL_Ventricle_MR_Baseline_11_40_SAMPLES': 'best_of_ATLAS_Ventricle_MR_Baseline_11',\n",
       "   'TL_Ventricle_MR_Baseline_11_60_SAMPLES': 'best_of_ATLAS_Ventricle_MR_Baseline_11'},\n",
       "  'CT': {'TL_Ventricle_CT_Baseline_1': 'best_of_ATLAS_Ventricle_MR_Baseline_1',\n",
       "   'TL_Ventricle_CT_Baseline_2': 'best_of_ATLAS_Ventricle_MR_Baseline_2',\n",
       "   'TL_Ventricle_CT_Baseline_3': 'best_of_ATLAS_Ventricle_MR_Baseline_3',\n",
       "   'TL_Ventricle_CT_Baseline_4': 'best_of_ATLAS_Ventricle_MR_Baseline_4',\n",
       "   'TL_Ventricle_CT_Baseline_5': 'best_of_ATLAS_Ventricle_MR_Baseline_5',\n",
       "   'TL_Ventricle_CT_Baseline_6': 'best_of_ATLAS_Ventricle_MR_Baseline_6',\n",
       "   'TL_Ventricle_CT_Baseline_7': 'best_of_ATLAS_Ventricle_MR_Baseline_7',\n",
       "   'TL_Ventricle_CT_Baseline_8': 'best_of_ATLAS_Ventricle_MR_Baseline_8',\n",
       "   'TL_Ventricle_CT_Baseline_9': 'best_of_ATLAS_Ventricle_MR_Baseline_9',\n",
       "   'TL_Ventricle_CT_Baseline_10': 'best_of_ATLAS_Ventricle_MR_Baseline_10',\n",
       "   'TL_Ventricle_CT_Baseline_11': 'best_of_ATLAS_Ventricle_MR_Baseline_11',\n",
       "   'TL_Ventricle_CT_Samples10_Baseline_11': 'best_of_ATLAS_Ventricle_MR_Baseline_11',\n",
       "   'TL_Ventricle_CT_Samples20_Baseline_11': 'best_of_ATLAS_Ventricle_MR_Baseline_11',\n",
       "   'TL_Ventricle_CT_Samples50_Baseline_11': 'best_of_ATLAS_Ventricle_MR_Baseline_11',\n",
       "   'TL_Ventricle_CT_Baseline_11_5_SAMPLES': 'best_of_ATLAS_Ventricle_MR_Baseline_11',\n",
       "   'TL_Ventricle_CT_Baseline_11_10_SAMPLES': 'best_of_ATLAS_Ventricle_MR_Baseline_11',\n",
       "   'TL_Ventricle_CT_Baseline_11_20_SAMPLES': 'best_of_ATLAS_Ventricle_MR_Baseline_11',\n",
       "   'TL_Ventricle_CT_Baseline_11_40_SAMPLES': 'best_of_ATLAS_Ventricle_MR_Baseline_11',\n",
       "   'TL_Ventricle_CT_Baseline_11_60_SAMPLES': 'best_of_ATLAS_Ventricle_MR_Baseline_11'}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weakly Supervised Transfer Learning Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.notebook.core import *\n",
    "import sys, os\n",
    "\n",
    "# add local/ package to python path to allow script to access py modules\n",
    "if not IN_NOTEBOOK: sys.path.insert(0, os.path.abspath(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.vision.all import *\n",
    "from fastai2.data.all import *\n",
    "from local.datasource import *\n",
    "from local.models import *\n",
    "from fastai2.torch_core import *\n",
    "from fastai2.basics import *\n",
    "from local.trainutils import *\n",
    "from fastai2.callback.all import *\n",
    "from fastai2.distributed import *\n",
    "from time import time\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " {'notl_brain_mr': functools.partial(<function create_mr_ct_dsource at 0x7f5ac68b3f80>, '/home/turgutluk/data/ventricles_data/mr', '/home/turgutluk/data/ventricles_data/csvs/mr_splits_df.csv', <function tfm_image_pt at 0x7f5b396a6200>, <function tfm_brain_mask_pt at 0x7f5ac68aaf80>),\n",
       "  'notl_brain_ct': functools.partial(<function create_mr_ct_dsource at 0x7f5ac68b3f80>, '/home/turgutluk/data/ventricles_data/ct', '/home/turgutluk/data/ventricles_data/csvs/ct_splits_df.csv', <function tfm_image_pt at 0x7f5b396a6200>, <function tfm_brain_mask_pt at 0x7f5ac68aaf80>),\n",
       "  'notl_ventricle_mr': functools.partial(<function create_mr_ct_dsource at 0x7f5ac68b3f80>, '/home/turgutluk/data/ventricles_data/mr', '/home/turgutluk/data/ventricles_data/csvs/mr_splits_df.csv', <function tfm_skull_stripped_image_pt at 0x7f5ac68b3dd0>, <function tfm_ventricles_mask_pt at 0x7f5ac68b3e60>),\n",
       "  'notl_ventricle_ct': functools.partial(<function create_mr_ct_dsource at 0x7f5ac68b3f80>, '/home/turgutluk/data/ventricles_data/ct', '/home/turgutluk/data/ventricles_data/csvs/ct_splits_df.csv', <function tfm_skull_stripped_image_pt at 0x7f5ac68b3dd0>, <function tfm_ventricles_mask_pt at 0x7f5ac68b3e60>),\n",
       "  'atlas_brain_mr': functools.partial(<function create_atlas_dsource at 0x7f5ac68b9290>, '/home/turgutluk/data/ventricles_data/atlas', '/home/turgutluk/data/ventricles_data/mr', '/home/turgutluk/data/ventricles_data/csvs/atlas_splits_df.csv', '/home/turgutluk/data/ventricles_data/csvs/mr_splits_df.csv', <function atlas_tfm_skull_stripped_image_pt at 0x7f5ac68b9170>, <function atlas_tfm_ventricles_mask_pt at 0x7f5ac68b9200>),\n",
       "  'atlas_ventricle_mr': functools.partial(<function create_atlas_dsource at 0x7f5ac68b9290>, '/home/turgutluk/data/ventricles_data/atlas', '/home/turgutluk/data/ventricles_data/mr', '/home/turgutluk/data/ventricles_data/csvs/atlas_splits_df.csv', '/home/turgutluk/data/ventricles_data/csvs/mr_splits_df.csv', <function atlas_tfm_image_pt at 0x7f5ac68b9050>, <function atlas_tfm_brain_mask_pt at 0x7f5ac68b90e0>)})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasource_dict), datasource_dict # all possible dataset for experiments, (atlas, mr, ct) x (brain, ventricle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,\n",
       " {'baseline1': functools.partial(<function unet_default at 0x7f5ac68b3d40>, p=0.0, norm_type='batch', actn='relu'),\n",
       "  'baseline2': functools.partial(<function unet_default at 0x7f5ac68b3d40>, p=0.0, norm_type='batch', actn='relu'),\n",
       "  'baseline3': functools.partial(<function unet_default at 0x7f5ac68b3d40>, p=0.0, norm_type='group', actn='relu'),\n",
       "  'baseline4': functools.partial(<function unet_default at 0x7f5ac68b3d40>, p=0.0, norm_type='group', actn='prelu'),\n",
       "  'baseline5': functools.partial(<function unet_default at 0x7f5ac68b3d40>, p=0.3, norm_type='group', actn='prelu'),\n",
       "  'baseline6': <function local.models.meshnet()>,\n",
       "  'baseline7': functools.partial(<function unet_wide at 0x7f5ac684bef0>, p=0.0, norm_type='group', actn='prelu'),\n",
       "  'baseline8': functools.partial(<function unet_deep at 0x7f5ac684bf80>, p=0.0, norm_type='group', actn='prelu'),\n",
       "  'baseline9': functools.partial(<function unet_wide_deep at 0x7f5ac684e050>, p=0.0, norm_type='group', actn='prelu'),\n",
       "  'baseline10': <function local.models.residual_unet()>,\n",
       "  'baseline11': <function local.models.residual_unet_wide()>})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment_model_dict), experiment_model_dict # all possible model configs for experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### args for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu=0\n",
    "\n",
    "data_name = \"notl_brain_mr\"\n",
    "sample_size = 10\n",
    "seed=42\n",
    "bs=2\n",
    "model_name='baseline1'\n",
    "MODEL_NAME='TL_Brain_MR_Baseline_1'\n",
    "model_dir='tl_brain_mr_models'\n",
    "loss_func='dice'\n",
    "\n",
    "TASK='BRAIN'\n",
    "MODALITY='MR'\n",
    "tl_model_path=\"atlas_brain_mr_models/ATLAS_Brain_MR_Baseline_1\" # relative path\n",
    "\n",
    "eps=1e-8\n",
    "epochs=2\n",
    "lr=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@contextmanager\n",
    "def np_local_seed(seed):\n",
    "    \"numpy local seed - doesn't effect global random state\"\n",
    "    state = np.random.get_state()\n",
    "    np.random.seed(seed)\n",
    "    try: yield\n",
    "    finally: np.random.set_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsource = datasource_dict[data_name]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_size:\n",
    "    with np_local_seed(seed):\n",
    "        dsource.splits[0] = L(np.random.choice(dsource.splits[0], sample_size))  # subsample training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([131,  68, 118,  20, 135,  91,  80,  28, 131, 106])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsource.splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = dsource.databunch(after_batch=[Cuda()], bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<fastai2.data.core.TfmdDL at 0x7f3669b3ec50>,\n",
       " <fastai2.data.core.TfmdDL at 0x7f36688f5e10>,\n",
       " <fastai2.data.core.TfmdDL at 0x7f36688f5fd0>,\n",
       " <fastai2.data.core.TfmdDL at 0x7f36688f5190>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbunch.dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create for test1 and test2 dls\n",
    "if len(dbunch.dls) == 4: pass\n",
    "elif len(dbunch.dls) == 2: pass\n",
    "else: raise Exception(f\"DataSource should have either 2 or 4 subsets, but have {len(dsource.splits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.torch_core import *\n",
    "from fastai2.basics import *\n",
    "from local.trainutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "m = experiment_model_dict[model_name]()\n",
    "apply_leaf(m, partial(my_cond_init, func=nn.init.kaiming_normal_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### callbacks \n",
    "\n",
    "> There is an issue with CSVLogger - not able to create directory when not existing\n",
    "\n",
    "> SaveModelCallback have problem with loading with distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.callback.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SaveModelCallback(TrackerCallback):\n",
    "    \"A `TrackerCallback` that saves the model's best during training and loads it at the end.\"\n",
    "    def __init__(self, monitor='valid_loss', comp=None, min_delta=0., fname='model', every_epoch=False, add_save=None, with_opt=False):\n",
    "        super().__init__(monitor=monitor, comp=comp, min_delta=min_delta)\n",
    "        store_attr(self, 'fname,every_epoch,add_save,with_opt')\n",
    "\n",
    "    def _save(self, name):\n",
    "        self.learn.save(name, with_opt=self.with_opt)\n",
    "        if self.add_save is not None:\n",
    "            with self.add_save.open('wb') as f: self.learn.save(f, with_opt=self.with_opt)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Compare the value monitored to its best score and save if best.\"\n",
    "        if self.every_epoch: self._save(f'{self.fname}_{self.epoch}')\n",
    "        else: #every improvement\n",
    "            super().after_epoch()\n",
    "            if self.new_best: self._save(f'{self.fname}')\n",
    "\n",
    "# loading is a problem in distributed\n",
    "#     def on_train_end(self, **kwargs):\n",
    "#         \"Load the best model.\"\n",
    "#         if not self.every_epoch: self.learn.load(f'{self.fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_cb = SaveModelCallback( monitor='dice_score', comp=np.greater, every_epoch=False,\n",
    "                        fname=f'best_of_{MODEL_NAME}')\n",
    "# csv_logger_cb = CSVLogger(fname=f'{experiment_id}_history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [TerminateOnNaNCallback(), save_model_cb]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learner\n",
    "\n",
    "> Best models are saved under `experiments/{model_dir}/{MODEL_NAME}` directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tl_brain_mr_models', 'TL_Brain_MR_Baseline_1')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir, MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_func = model_split_dict[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai2.learner.Learner at 0x7f36688dd250>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf = loss_dict[loss_func]\n",
    "opt_func = partial(Adam, eps=eps)\n",
    "learn = Learner(dbunch, m, lf, metrics=[dice_score], opt_func=opt_func,\n",
    "                path=Path('experiments'),\n",
    "                model_dir=Path(model_dir)/MODEL_NAME, cbs=callbacks, splitter=split_func)\n",
    "learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('experiments'),\n",
       " PosixPath('tl_brain_mr_models/TL_Brain_MR_Baseline_1'))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.path, learn.model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(os.environ.get('YAML_TL', 'transfer_learning.yaml')) as f: tl = yaml.load(f.read(), yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "path, model_dir = learn.path, learn.model_dir\n",
    "tl_model_name = tl[TASK][MODALITY][MODEL_NAME]\n",
    "learn.path, learn.model_dir = path, tl_model_path\n",
    "learn.load(tl_model_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('experiments'),\n",
       " PosixPath('tl_brain_mr_models/TL_Brain_MR_Baseline_1'))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.path, learn.model_dir = path, model_dir\n",
    "learn.path, learn.model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_groups = len(learn.opt.param_groups)\n",
    "tl_epochs = array([epochs]*n_groups)//np.power(2, array(list(range(n_groups))))\n",
    "tl_epochs = np.clip(tl_epochs,1,np.inf).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 1, 1], [0, 1, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tl_epochs), list(range(n_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing to param group: -1 and training for 2 epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.754202</td>\n",
       "      <td>0.761797</td>\n",
       "      <td>0.102429</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.730773</td>\n",
       "      <td>0.768464</td>\n",
       "      <td>0.115393</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing to param group: -2 and training for 1 epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.652640</td>\n",
       "      <td>0.903330</td>\n",
       "      <td>0.051248</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing to param group: -3 and training for 1 epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.544313</td>\n",
       "      <td>0.806283</td>\n",
       "      <td>0.138899</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, _epochs in zip(range(1, n_groups+1), tl_epochs):\n",
    "    if not int(gpu): print(f\"Freezing to param group: {-i} and training for {_epochs} epochs\")\n",
    "    learn.freeze_to(-i)\n",
    "    learn.fit_one_cycle(_epochs, slice(lr), cbs=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'best_of_{MODEL_NAME}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.cbs = [cb for cb in learn.cbs if not isinstance(cb, TrackerCallback) and\n",
    "                                       not isinstance(cb, TerminateOnNaNCallback)]\n",
    "# learn.validate(1) # debug - best model is saved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('tl_brain_mr_models/TL_Brain_MR_Baseline_1')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save test1 and test2 results for unique experiment at time.time()\n",
    "if len(dbunch.dls) == 4: \n",
    "    test1_eval, test2_eval = learn.validate(2), learn.validate(3)\n",
    "    eval_dir = f\"test_results/{model_dir}\"\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "    save_fn = f\"{eval_dir}/{str(int(time()))}.txt\"\n",
    "    with open(save_fn, 'w') as f: f.write(str([test1_eval, test2_eval]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('experiments')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('tl_brain_mr_models/TL_Brain_MR_Baseline_1'),\n",
       " 'TL_Brain_MR_Baseline_1')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir, MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `script` - Don't execute cells below!\n",
    "\n",
    "These cells are converted into a python script in `training_scripts/traintransfer.py` which you can use instead of running in noteboook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"notl_brain_mr\"\n",
    "sample_size = 10\n",
    "seed=42\n",
    "bs=2\n",
    "model_name='baseline1'\n",
    "MODEL_NAME='TL_Brain_MR_Baseline_1'\n",
    "model_dir='tl_brain_mr_models'\n",
    "loss_func='dice'\n",
    "TASK='BRAIN'\n",
    "MODALITY='MR'\n",
    "tl_model_path=\"atlas_brain_mr_models/ATLAS_Brain_MR_Baseline_1\" # relative path\n",
    "eps=1e-8\n",
    "epochs=2\n",
    "lr=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "@call_parse\n",
    "def main(\n",
    "    gpu:Param(\"GPU to run on\", str)=None,\n",
    "    data_name:Param(\"Data name for experiment\", str)=\"notl_brain_mr\",\n",
    "    sample_size:Param(\"Random samples for training, default None - full\", int)=None,\n",
    "    seed:Param(\"Random seed for sample_size\", int)=None,\n",
    "    bs:Param(\"Batch size for training\", int)=2,\n",
    "    model_name:Param(\"Model architecture config - baseline*\", str)=\"baseline1\",\n",
    "    MODEL_NAME:Param(\"Model name to save the model\", str)=\"TL_Brain_MR_Baseline_1\",\n",
    "    model_dir:Param(\"Directory to save model\", str)=\"tl_brain_mr_models\",\n",
    "    loss_func:Param(\"Loss function for training\", str)='dice',\n",
    "    TASK:Param(\"Task defined for transfer learning in tl.yaml\", str)='BRAIN',\n",
    "    MODALITY:Param(\"Modality defined for transfer learning in tl.yaml\", str)='MR',\n",
    "    tl_model_path:Param(\"Relative model path\", str)=\"atlas_brain_mr_models/ATLAS_Brain_MR_Baseline_1\", \n",
    "    eps:Param(\"Eps value for Adam optimizer\", float)=1e-8,\n",
    "    epochs:Param(\"Number of epochs for training\", int)=2,\n",
    "    lr:Param(\"Learning rate for training\", float)=0.1):\n",
    "    \n",
    "    \"Distributed de novo training - aka. from scratch\"\n",
    "    import os; print(os.getcwd())\n",
    "\n",
    "    gpu = setup_distrib(gpu)\n",
    "    n_gpus, gpu_rank = num_distrib(), rank_distrib()\n",
    "\n",
    "    # data\n",
    "    dsource = datasource_dict[data_name]()\n",
    "    if sample_size:\n",
    "        with np_local_seed(seed):\n",
    "            dsource.splits[0] = L(np.random.choice(dsource.splits[0], sample_size))  # subsample training\n",
    "    dbunch = dsource.databunch(after_batch=[Cuda()], bs=bs)\n",
    "    if len(dbunch.dls) == 4: pass\n",
    "    elif len(dbunch.dls) == 2: pass\n",
    "    else: raise Exception(f\"DataSource should have either 2 or 4 subsets, but have {len(dsource.splits)}\")\n",
    "\n",
    "    # model\n",
    "    m = experiment_model_dict[model_name]()\n",
    "    apply_leaf(m, partial(my_cond_init, func=nn.init.kaiming_normal_))\n",
    "    \n",
    "    # callbacks\n",
    "    save_model_cb = SaveModelCallback(monitor='dice_score', comp=np.greater, every_epoch=False,\n",
    "                        fname=f'best_of_{MODEL_NAME}')\n",
    "    callbacks = [TerminateOnNaNCallback(), save_model_cb]        \n",
    "    \n",
    "    # learn\n",
    "    split_func = model_split_dict[model_name]\n",
    "    lf = loss_dict[loss_func]\n",
    "    opt_func = partial(Adam, eps=eps)\n",
    "    learn = Learner(dbunch, m, lf, metrics=[dice_score], opt_func=opt_func,\n",
    "                    path=Path('experiments'),\n",
    "                    model_dir=Path(model_dir)/MODEL_NAME, cbs=callbacks, splitter=split_func)\n",
    "    learn.to_fp16()\n",
    "\n",
    "    # load pretrained\n",
    "    with open(os.environ.get('YAML_TL', 'transfer_learning.yaml')) as f: \n",
    "        tl = yaml.load(f.read(), yaml.FullLoader)\n",
    "    path, model_dir = learn.path, learn.model_dir\n",
    "    tl_model_name = tl[TASK][MODALITY][MODEL_NAME]\n",
    "    learn.path, learn.model_dir = path, tl_model_path\n",
    "    learn.load(tl_model_name);\n",
    "    learn.path, learn.model_dir = path, model_dir\n",
    "    \n",
    "    # distributed\n",
    "    if gpu is None:       learn.to_parallel()\n",
    "    elif num_distrib()>1: learn.to_distributed(gpu)    \n",
    "\n",
    "    # fine tuning - transfer learning\n",
    "    n_groups = len(learn.opt.param_groups)\n",
    "    tl_epochs = array([epochs]*n_groups)//np.power(2, array(list(range(n_groups))))\n",
    "    tl_epochs = np.clip(tl_epochs,1,np.inf).astype(int)\n",
    "    for i, _epochs in zip(range(1, n_groups+1), tl_epochs):\n",
    "        if not int(gpu): print(f\"Freezing to param group: {-i} and training for {_epochs} epochs\")\n",
    "        learn.freeze_to(-i)\n",
    "        learn.fit_one_cycle(_epochs, slice(lr), cbs=callbacks)\n",
    "    \n",
    "\n",
    "\n",
    "    # evaluate\n",
    "    if not gpu_rank:\n",
    "        if len(dbunch.dls) == 4: \n",
    "            learn.load(f'best_of_{MODEL_NAME}');\n",
    "            learn.cbs = [cb for cb in learn.cbs if not isinstance(cb, TrackerCallback) and\n",
    "                                               not isinstance(cb, TerminateOnNaNCallback)]        \n",
    "            test1_eval, test2_eval = learn.validate(2), learn.validate(3)\n",
    "            eval_dir = f\"test_results/{model_dir}\"\n",
    "            os.makedirs(eval_dir, exist_ok=True)\n",
    "            save_fn = f\"{eval_dir}/{str(int(time()))}.txt\"\n",
    "            with open(save_fn, 'w') as f: f.write(str([test1_eval, test2_eval]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special export to fix for relative import problems\n",
    "from local.notebook.export import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_nb_to(src, dest):\n",
    "    \"export src notebook to dest\"\n",
    "    cells = read_nb(src)['cells']\n",
    "    srcs = []\n",
    "    for cell in cells: \n",
    "        src = cell['source']\n",
    "        if src.startswith(\"#export\"): srcs.append(src+\"\\n\") \n",
    "        if src.startswith(\"# export\"): srcs.append(src+\"\\n\")\n",
    "    res = \"\\n\".join(srcs)\n",
    "    with open(dest, \"w\") as f: f.write(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_nb_to(\"3c) traintransfer.ipynb\", \"training_scripts/traintransfer.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `[notebook run #1]` - training in weakly supervised transfer learning mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fastai2.basics import *\n",
    "import fastai2\n",
    "from local.scriptrunner import *\n",
    "\n",
    "launch_file = f\"{Path(fastai2.__file__).parent}/launch.py\"\n",
    "script = \"./training_scripts/traintransfer.py \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributed training with multiple gpus\n",
    "# change parameters accordingly\n",
    "cmd = f\"\"\"python {launch_file} \\\n",
    "--gpus=12 {script} \\\n",
    "--data_name=notl_brain_mr \\\n",
    "--sample_size=10 \\\n",
    "--seed=42 \\\n",
    "--bs=2 \\\n",
    "--model_name=baseline1 \\\n",
    "--MODEL_NAME=TL_Brain_MR_Baseline_1 \\\n",
    "--model_dir=tl_brain_mr_models \\\n",
    "--loss_func=bce \\\n",
    "--TASK=BRAIN \\\n",
    "--MODALITY=MR \\\n",
    "--tl_model_path=atlas_brain_mr_models/ATLAS_Brain_MR_Baseline_1 \\\n",
    "--eps=1e-8 \\\n",
    "--epochs=2 \\\n",
    "--lr=0.1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single gpu training \n",
    "# change parameters accordingly\n",
    "cmd = f\"\"\"python {script} \\\n",
    "--gpu=0 \\\n",
    "--data_name=notl_brain_mr \\\n",
    "--sample_size=10 \\\n",
    "--seed=42 \\\n",
    "--bs=2 \\\n",
    "--model_name=baseline1 \\\n",
    "--MODEL_NAME=TL_Brain_MR_Baseline_1 \\\n",
    "--model_dir=tl_brain_mr_models \\\n",
    "--loss_func=bce \\\n",
    "--TASK=BRAIN \\\n",
    "--MODALITY=MR \\\n",
    "--tl_model_path=atlas_brain_mr_models/ATLAS_Brain_MR_Baseline_1 \\\n",
    "--eps=1e-8 \\\n",
    "--epochs=2 \\\n",
    "--lr=0.1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python /home/turgutluk/fastai2_fork/fastai2/launch.py --gpus=12 ./training_scripts/traintransfer.py  --data_name=notl_brain_mr --sample_size=10 --seed=42 --bs=2 --model_name=baseline1 --MODEL_NAME=TL_Brain_MR_Baseline_1 --model_dir=tl_brain_mr_models --loss_func=bce --TASK=BRAIN --MODALITY=MR --tl_model_path=atlas_brain_mr_models/ATLAS_Brain_MR_Baseline_1 --eps=1e-8 --epochs=2 --lr=0.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"./training_scripts/traintransfer.py\", line 11, in <module>\n",
      "    from local.datasource import *\n",
      "  File \"/home/turgutluk/Vent_Seg_Project/dev/local/datasource.py\", line 16, in <module>\n",
      "    with open(os.environ.get('YAML_DATA', '../data.yaml')) as f: data_config = yaml.load(f.read(), yaml.FullLoader)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data.yaml'\n",
      "Traceback (most recent call last):\n",
      "  File \"./training_scripts/traintransfer.py\", line 11, in <module>\n",
      "    from local.datasource import *\n",
      "  File \"/home/turgutluk/Vent_Seg_Project/dev/local/datasource.py\", line 16, in <module>\n",
      "    with open(os.environ.get('YAML_DATA', '../data.yaml')) as f: data_config = yaml.load(f.read(), yaml.FullLoader)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data.yaml'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_command(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai_dev]",
   "language": "python",
   "name": "conda-env-fastai_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
